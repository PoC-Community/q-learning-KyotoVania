{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "### Introduction\n",
    "Today, you'll learn about QLearning using the gym api.\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments.\n",
    "\n",
    "Reinforcement learning is a subset of Artificial Intelligence. Our AI will learn by playing the same game over and over again until it learns what it must do in order to win.\n",
    "\n",
    "In this workshop, you will:\n",
    "\n",
    "1. Learn about Q Tables\n",
    "2. Setup a gym environment\n",
    "3. Train an AI to solve the [FrozenLake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (0.28.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (from gymnasium[toy-text]) (1.23.5)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (from gymnasium[toy-text]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (from gymnasium[toy-text]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (from gymnasium[toy-text]) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: pygame==2.1.3 in c:\\users\\jeanc\\documents\\github\\q-learning-kyotovania\\venv\\lib\\site-packages (from gymnasium[toy-text]) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "!{sys.executable} -m pip install gymnasium[toy-text]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.707329600Z",
     "start_time": "2023-11-28T17:32:55.340429100Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of Q-Learning: \n",
    "\n",
    "![Example of a Q-Learning Environment](./images/example.png)\n",
    "\n",
    "A game where the AI must get to the ‚ÄúEnd‚Äù coordinates on the map in the shortest amount of time while avoiding the bombs and collecting the bonuses.\n",
    "\n",
    "A Q-Table for this environment could be visualized like this. \n",
    "\n",
    "![Example of a Q-Table](./images/qtable.png)\n",
    "\n",
    "With all of the possible actions listed horizontally and all of the possible states listed vertically.\n",
    "\n",
    "Write a function which generates an empty array of shape `x` * `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.708327Z",
     "start_time": "2023-11-28T17:32:57.701282800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def init_q_table(x: int, y: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns a 2D matrix containing only zeros for values.\n",
    "    The shape of the matrix is defined by the parameters x and y.\n",
    "    \"\"\"\n",
    "    return np.zeros((x, y))\n",
    "\n",
    "qTable = init_q_table(5, 4)\n",
    "\n",
    "print(\"Q-Table:\\n\" + str(qTable))\n",
    "\n",
    "assert(np.mean(qTable) == 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the AI receives a **reward** each time it **acts**:\n",
    "\n",
    "This reward is a number that will be interpreted by our AI:\n",
    "\n",
    "- If it is negative, the AI will learn that the action it made which led to this reward was probably not the best\n",
    "- If it is null or positive, the AI will learn that the action it made which led to this reward was probably smart and it should do it again.\n",
    "\n",
    "Here, the AI would likely receive a **negative reward** (-1) if it chooses to move to the right (because there's a bomb)\n",
    "\n",
    "However, if it went anywhere else, it would probably receive a **neutral reward** (0), since it would end up on blank squares.\n",
    "\n",
    "Assuming it decided to go to the right, the AI would encounter a bomb and the value for [Start, Move_Right] would decrease.\n",
    "\n",
    "\n",
    "Once we receive a reward, we can use the Q Formula below to update our Q Table's values. Our AI will use these values to learn what the most profitable action is at each state of the environment.\n",
    "\n",
    "![QFormula](./images/qformula.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "\n",
    "**Q<sup>new</sup>(s<sub>t</sub>, a<sub>t</sub>)** = the new value for the current State and Action on our Qtable\n",
    "\n",
    "**Q(s<sub>t</sub>, a<sub>t</sub>)** = the old value for the current State and Action on our QTable\n",
    "\n",
    "**Œ± (learning rate)** = a constant value we use for our algorithm's learning speed (usually a number between 0.5 and 0.05)\n",
    "\n",
    "**r<sub>t</sub> (reward)** = the reward received (as per the example above, -1 if you hit a bomb)\n",
    "\n",
    "**ùõæ (discount factor)** = another constant value we use to determine how good of a memory our AI has\n",
    "\n",
    "**max Q<sup>new</sup>(s<sub>t+1</sub>, a)** = the value of the most profitable action at the new state\n",
    "\n",
    "##### Now that you know how a Q Table updates itself using the Q Formula, let's try to implement this formula as a python function:\n",
    "(You can use the `LEARNING_RATE` and `DISCOUNT_RATE` constants in your formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "DISCOUNT_RATE = 0.99\n",
    "\n",
    "def q_function(q_table: np.ndarray, state: int, action: int, reward: int, newState: int) -> float:\n",
    "    \"\"\"\n",
    "    Implement the Q-Learning formula to update the Q-table.\n",
    "    \"\"\"\n",
    "    old_value = q_table[state, action]\n",
    "    future_max = np.max(q_table[newState])\n",
    "    new_value = old_value + LEARNING_RATE * (reward + DISCOUNT_RATE * future_max - old_value)\n",
    "    \n",
    "    # Update the Q-table with the new value\n",
    "    q_table[state, action] = new_value\n",
    "\n",
    "    return new_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.722201300Z",
     "start_time": "2023-11-28T17:32:57.709326600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can test the Q Function to see if it works !\\\n",
    "If qTable[0, 1]'s value below is `-0.05`, congrats: you have successfully implemented the formula !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table after action:\n",
      "[[ 0.   -0.05  0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "q_table = init_q_table(5,4)\n",
    "\n",
    "q_table[0, 1] = q_function(q_table, state=0, action=1, reward=-1, newState=3)\n",
    "\n",
    "print(\"Q-Table after action:\\n\" + str(q_table))\n",
    "\n",
    "assert(q_table[0, 1] == -LEARNING_RATE), f\"The Q function is incorrect: the value of qTable[0, 1] should be -{LEARNING_RATE}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.723199300Z",
     "start_time": "2023-11-28T17:32:57.715934300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Setting up the GYM Environment\n",
    "\n",
    "#### What is Gym ?\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments\n",
    ">\n",
    "> -- <cite>Gym Docs</cite>\n",
    "\n",
    "Let's get to the fun part:\n",
    "Read the documentation for FrozenLake [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "and find out how to load the environment.\n",
    "\n",
    "For now, we will set the `is_slippery` variable to `False` \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.804351300Z",
     "start_time": "2023-11-28T17:32:57.724196800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# write some code to load and make the FrozenLake environment:\n",
    "import gymnasium as gym\n",
    "\n",
    "# Load and create the FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode='human')\n",
    "\n",
    "# You can optionally print the environment's action space and observation space to understand it better\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our environment is loaded, we need a function that performs a random action.\n",
    "\n",
    "An action is a number between 0 and the total number of possible actions.\n",
    "\n",
    "Try to find the value (inside `env`) that gives this total number of actions in the environment and store it in `total_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.804351300Z",
     "start_time": "2023-11-28T17:32:57.733187Z"
    }
   },
   "outputs": [],
   "source": [
    "total_actions = env.action_space.n\n",
    "\n",
    "assert(total_actions == 4), f\"There are a total of four possible actions in this environment. Your answer is {total_actions}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `total_actions` to make a function which returns a random action each time it is called !\n",
    "\n",
    "There is also another way to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:57.805850900Z",
     "start_time": "2023-11-28T17:32:57.737772600Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_action(env):\n",
    "    return random.randint(0, env.action_space.n - 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `random_action` function, you can run the code below which will display the first frame of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:58.765192700Z",
     "start_time": "2023-11-28T17:32:57.743201900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 4\n",
      "states: 16\n",
      "Current state: 1\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "# Performing an action\n",
    "action = random_action(env)\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "\n",
    "# Displaying the first frame of the game\n",
    "env.render()\n",
    "\n",
    "# Printing game info\n",
    "print(f\"actions: {env.action_space.n}\\nstates: {env.observation_space.n}\")\n",
    "print(f\"Current state: {observation}\")\n",
    "\n",
    "# Closing the environment\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, there are **4 possible actions** for each of the **16 possible states**.\\\n",
    "Feel free to play around with the code above to get a better understanding of the API.\n",
    "\n",
    "## 3. Solving the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a Q Table, Q Function and our environment, we can create our `game_loop`.\n",
    "\n",
    "Call `qFunc()` using the correct arguments it requires in the code below to update the `qTable[,]` using the values available in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:32:58.779459Z",
     "start_time": "2023-11-28T17:32:58.767214400Z"
    }
   },
   "outputs": [],
   "source": [
    "def game_loop(env: gym.Env, q_table: np.ndarray, state: int, action: int) -> tuple:\n",
    "    # Perform the action in the environment\n",
    "    new_state, reward, done, extra_boolean, info = env.step(action)  # Adjusted unpacking\n",
    "\n",
    "    # Update the Q-table using the Q-learning formula\n",
    "    old_value = q_table[state, action]\n",
    "    future_max = np.max(q_table[new_state])\n",
    "    new_value = old_value + LEARNING_RATE * (reward + DISCOUNT_RATE * future_max - old_value)\n",
    "    q_table[state, action] = new_value\n",
    "\n",
    "    # Return the updated Q-table, new state, done flag, and reward\n",
    "    return q_table, new_state, done, reward\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how the environment runs using your `game_loop`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:33:05.902216Z",
     "start_time": "2023-11-28T17:32:58.773534700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0, False, False, {'prob': 1.0})\n",
      "(1, 0.0, False, False, {'prob': 1.0})\n",
      "(1, 0.0, False, False, {'prob': 1.0})\n",
      "(4, 0.0, False, False, {'prob': 1.0})\n",
      "(8, 0.0, False, False, {'prob': 1.0})\n",
      "(8, 0.0, False, False, {'prob': 1.0})\n",
      "(8, 0.0, False, False, {'prob': 1.0})\n",
      "(9, 0.0, False, False, {'prob': 1.0})\n",
      "(11, 0.0, True, False, {'prob': 1.0})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "state, info = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = random_action(env)\n",
    "\n",
    "    # Add a print statement to inspect the output of env.step(action)\n",
    "    step_result = env.step(action)\n",
    "    print(step_result)\n",
    "\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "Now, to see if our AI learns, we will launch the environment 1000 times and see how the Q-Table evolves.\n",
    "\n",
    "Notice how in the code below, we do not call `env.render()` each frame because if we did, our 1000 iterations would take a **lot** of time to complete."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:33:08.095775Z",
     "start_time": "2023-11-28T17:33:05.904210500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0m\u001B[92m0.941\t\u001B[0m\u001B[92m0.951\t\u001B[4m\u001B[92m0.951\t\u001B[0m\u001B[92m0.941\t\n",
      "\u001B[0m\u001B[92m0.941\t\u001B[0m\u001B[00m0.0\t\u001B[4m\u001B[92m0.961\t\u001B[0m\u001B[92m0.951\t\n",
      "\u001B[0m\u001B[92m0.951\t\u001B[4m\u001B[92m0.97\t\u001B[0m\u001B[92m0.951\t\u001B[0m\u001B[92m0.961\t\n",
      "\u001B[4m\u001B[92m0.961\t\u001B[0m\u001B[00m0.0\t\u001B[0m\u001B[92m0.951\t\u001B[0m\u001B[92m0.951\t\n",
      "\u001B[0m\u001B[92m0.951\t\u001B[4m\u001B[92m0.961\t\u001B[0m\u001B[00m0.0\t\u001B[0m\u001B[92m0.941\t\n",
      "\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\n",
      "\u001B[0m\u001B[00m0.0\t\u001B[4m\u001B[92m0.98\t\u001B[0m\u001B[00m0.0\t\u001B[0m\u001B[92m0.961\t\n",
      "\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\n",
      "\u001B[0m\u001B[92m0.961\t\u001B[0m\u001B[00m0.0\t\u001B[4m\u001B[92m0.97\t\u001B[0m\u001B[92m0.951\t\n",
      "\u001B[0m\u001B[92m0.961\t\u001B[0m\u001B[92m0.98\t\u001B[4m\u001B[92m0.98\t\u001B[0m\u001B[00m0.0\t\n",
      "\u001B[0m\u001B[92m0.97\t\u001B[4m\u001B[92m0.99\t\u001B[0m\u001B[00m0.0\t\u001B[0m\u001B[92m0.97\t\n",
      "\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\n",
      "\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\n",
      "\u001B[0m\u001B[00m0.0\t\u001B[0m\u001B[92m0.98\t\u001B[4m\u001B[92m0.99\t\u001B[0m\u001B[92m0.97\t\n",
      "\u001B[0m\u001B[92m0.98\t\u001B[0m\u001B[92m0.99\t\u001B[4m\u001B[92m1.0\t\u001B[0m\u001B[92m0.98\t\n",
      "\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\u001B[4m\u001B[00m0.0\t\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 20000\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        # This time, we won't render the game each frame because it would take too long\n",
    "        action = random_action(env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "# Printing the QTable result:\n",
    "for states in q_table:\n",
    "    for actions in states:\n",
    "        if (actions == max(states)):\n",
    "            print(\"\\033[4m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[0m\", end=\"\")\n",
    "        if (actions > 0):\n",
    "            print(\"\\033[92m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[00m\", end=\"\")\n",
    "        print(round(actions, 3), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We now have a nice Q-Table that indicates which action is best for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, it would probably be better for our agent to choose actions based on its experience, rather than at random.\n",
    "Write a function which chooses the best action for each given state:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:33:08.095775Z",
     "start_time": "2023-11-28T17:33:08.090762600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def best_action(q_table: np.ndarray, state: int) -> int:\n",
    "    \"\"\"\n",
    "    Finds the best action for the given state based on the Q-table.\n",
    "\n",
    "    Args:\n",
    "    q_table: The Q-table, a 2D numpy array.\n",
    "    state: The current state.\n",
    "\n",
    "    Returns:\n",
    "    The index of the action with the highest Q-value for the given state.\n",
    "    \"\"\"\n",
    "    return np.argmax(q_table[state])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the result of our training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:33:11.487318700Z",
     "start_time": "2023-11-28T17:33:08.094269300Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = best_action(q_table, state)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If all went well, our AI should easily reach its goal !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the AI should probably try to explore all the different possibilities before it starts optimising its gain by following the Q Table...\\\n",
    "In order to solve this problem, we can use the Epsilon-Greedy strategy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Epsilon Greedy ?\n",
    "\n",
    "Epsilon greedy is a strategy used to make sure that our AI explores its environment and doesn‚Äôt miss out on some cool easter eggs !\n",
    "\n",
    "![EpsilonGreedy](https://steadfast-ragdoll-d83.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa8444925-9940-46b2-b9e3-aae5ca792a74%2FUntitled.png?table=block&id=ce90b632-5d78-4cb0-8101-b326b91bf25d&spaceId=a008bb22-92df-498d-b32c-15988ddb70b9&width=770&userId=&cache=v2)\n",
    "\n",
    "<cite>[source](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870)</cite>\n",
    "\n",
    "```\n",
    "if random(0, 1) > epsilon-greedy:\n",
    "\treturn bestKnownAction\n",
    "else:\n",
    "\treturn randomAction\n",
    "```\n",
    "\t\t\n",
    "The higher the epsilon-greedy value is (from 0 to 1), the higher chance of the AI picking an action at random.\n",
    "\n",
    "The lower the epsilon-greedy value is (from 0 to 1 still), the higher chance of the AI picking an action it considers the most rewarding.\n",
    "\n",
    "In other words, with a high epsilon-greedy value, the robot pictured above might go to the right whereas it would probably choose the left path if it had a lower epsilon-greedy value, therefore missing out on a special discovery !\n",
    "\n",
    "\n",
    "\n",
    "Try to implement it into the following function that we'll use to determine which action the AI will choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:33:11.488313700Z",
     "start_time": "2023-11-28T17:33:11.483166Z"
    }
   },
   "outputs": [],
   "source": [
    "def choose_action(epsilon: float, q_table: np.ndarray, state: int, env: gym.Env) -> int:\n",
    "    \"\"\"\n",
    "    Write a function to either return a random action or the best action using the functions\n",
    "    defined earlier.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, the AI will explore all possible actions before it chooses the best state, reducing the risk of it missing some golden solutions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we're not done...\\\n",
    "You might have noticed that when we load our environment, we give it a certain argument:\\\n",
    "`is_slippery=False`\n",
    "\n",
    "This argument makes the game far easier !\n",
    "If you want a real challenge, set it to true."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the environment will be much harder to solve if the ice is slippery. Our agent's movements will be unpredictable and it will be impossible to make a 100% accurate AI.\n",
    "\n",
    "Therefore, keep track of the number of times the AI gets to its goal during our testing phase.\n",
    "\n",
    "##### Try to get the highest accuracy possible !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T17:33:11.873892900Z",
     "start_time": "2023-11-28T17:33:11.489311500Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     11\u001B[0m     action \u001B[38;5;241m=\u001B[39m choose_action(epsilon, q_table, state, env)\n\u001B[1;32m---> 12\u001B[0m     q_table, state, done, reward \u001B[38;5;241m=\u001B[39m \u001B[43mgame_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m     14\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m, in \u001B[0;36mgame_loop\u001B[1;34m(env, q_table, state, action)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgame_loop\u001B[39m(env: gym\u001B[38;5;241m.\u001B[39mEnv, q_table: np\u001B[38;5;241m.\u001B[39mndarray, state: \u001B[38;5;28mint\u001B[39m, action: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;66;03m# Perform the action in the environment\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m     new_state, reward, done, extra_boolean, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Adjusted unpacking\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# Update the Q-table using the Q-learning formula\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     old_value \u001B[38;5;241m=\u001B[39m q_table[state, action]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\q-learning-KyotoVania\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[0;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     55\u001B[0m \n\u001B[0;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\q-learning-KyotoVania\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\q-learning-KyotoVania\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:47\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menv_step_passive_checker\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\q-learning-KyotoVania\\venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:237\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[1;34m(env, action)\u001B[0m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001B[39;00m\n\u001B[1;32m--> 237\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m    239\u001B[0m     result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[0;32m    240\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects step result to be a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\q-learning-KyotoVania\\venv\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:301\u001B[0m, in \u001B[0;36mFrozenLakeEnv.step\u001B[1;34m(self, a)\u001B[0m\n\u001B[0;32m    300\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, a):\n\u001B[1;32m--> 301\u001B[0m     transitions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mP\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43ma\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    302\u001B[0m     i \u001B[38;5;241m=\u001B[39m categorical_sample([t[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m transitions], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnp_random)\n\u001B[0;32m    303\u001B[0m     p, s, r, t \u001B[38;5;241m=\u001B[39m transitions[i]\n",
      "\u001B[1;31mKeyError\u001B[0m: None"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "# Training the AI\n",
    "epsilon = 1.0\n",
    "for i in range(10000):\n",
    "    epsilon = max(epsilon - 0.0001, 0)\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(epsilon, q_table, state, env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "# Testing the AI\n",
    "wins = 0.0\n",
    "for i in range(100):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(0, q_table, state, env)\n",
    "        _, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            if (reward > 0):\n",
    "                wins += 1\n",
    "            break\n",
    "\n",
    "print(f\"{round(wins / (i+1) * 100, 2)}% winrate\")\n",
    "print(wins)\n",
    "\n",
    "# Displaying the last frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job ! You completed this workshop !\n",
    "If you want to continue in the wonderful world of reinforcement learning, you could try:\n",
    "- Try and increase the accuracy by using different methods of reinforcement learning.\n",
    "- A custom map for Frozen Lake. For example: `gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)`\n",
    "- A different environment from https://gymnasium.farama.org/\n",
    "- Going **deeper** with https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "- Trying what you've learned with games you know and love:\n",
    "    - https://gymnasium.farama.org/environments/atari/\n",
    "    - https://pypi.org/project/gym-super-mario-bros/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cba5719d159acad27bb7f37d182d27b1d38df28115fbd0d331121e7aca3605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
